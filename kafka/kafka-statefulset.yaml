apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: kafka
  name: kafka
  namespace: datahub
spec:
  serviceName: kafka
  replicas: 3  # Increased to 3 brokers
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        #network/kafka-network: "true"
        app: kafka
    spec:
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
      enableServiceLinks: false
      terminationGracePeriodSeconds: 300
      containers:
      - name: kafka
        imagePullPolicy: IfNotPresent
        image: confluentinc/cp-kafka:7.8.0
        ports:
          - containerPort: 29092
          - containerPort: 9092
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KAFKA_BROKER_ID  # Set dynamic broker ID
            value: "0"
          - name: KAFKA_ZOOKEEPER_CONNECT
            value: "zookeeper.datahub.svc.cluster.local:2181"
          - name: KAFKA_LISTENERS
            value: "PLAINTEXT://:9092,INTERNAL://:29092"
          - name: KAFKA_ADVERTISED_LISTENERS
            value: "PLAINTEXT://$(POD_NAME).kafka.datahub.svc.cluster.local:9092,INTERNAL://$(POD_NAME).kafka.datahub.svc.cluster.local:29092"
          - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
            value: "PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT"
          - name: KAFKA_INTER_BROKER_LISTENER_NAME
            value: "INTERNAL"
          - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR  # Can survive 2 broker failures, balanced durability & performance.
            value: "3"
          - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
            value: "2"
          - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
            value: "3"
          - name: KAFKA_SOCKET_REQUEST_MAX_BYTES
            value: "2000000000"
          - name: KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS
            value: "100"
          - name: KAFKA_MIN_INSYNC_REPLICAS # Ensures at least 2 replicas must acknowledge a write.
            value: "2"
          - name: KAFKA_LOG_RETENTION_HOURS
            value: "168" #Logs retained for 7 days (168 hours).
          - name: KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS # Log cleanup checks every 5 minutes.
            value: "300000"
        command:
          - "sh"
          - "-c"
          - |
            export KAFKA_BROKER_ID=$(echo ${POD_NAME##*-})
            exec /etc/confluent/docker/run
        volumeMounts:
          - mountPath: /var/lib/kafka
            name: kafka-data
      restartPolicy: Always
  volumeClaimTemplates:
    - metadata:
        name: kafka-data
      spec:
        storageClassName: sc-local
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
---
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: datahub
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - name: internal
      port: 29092
      targetPort: 29092
    - name: external
      port: 9092
      targetPort: 9092
